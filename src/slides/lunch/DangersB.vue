<script setup></script>
<template>
    <article>
        <h1>Dangers - Relationships</h1>
        <ul>
            <li>
                LLMs are good enough at pretending to be human that users start treating them like a person.
            </li>
            <li>
                At least one teenager has [allegedly] been killed by an LLM. After speaking with the AI for several months it encouraged their suicide.
                <ul>
                    <li>https://www.nbcnews.com/tech/characterai-lawsuit-florida-teen-death-rcna176791</li>
                </ul>
            </li>
            <li>
                People going through a manic episode or experiencing delusions can have it reinforced by an LLM.
                <ul>
                    <li>The tendency to agree with the user and try to please them can be dangerous.</li>
                </ul>
            </li>
            <li>
                AI companies are already optimising models towards keeping users engaged.
                <ul>
                    <li>As budgets tighten, charging money to keep talking to "your friend" may become a common business model.</li>
                </ul>
            </li>
        </ul>
    </article>
</template>
<style lang="scss" scoped>
article {
    display: flex;
    flex-direction: column;
    justify-content: center;
    align-items: center;

    padding: 0 80px;

    > ul > li {
        margin-bottom: 2rem;
    }
}
</style>